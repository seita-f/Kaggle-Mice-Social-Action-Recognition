{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59156,"databundleVersionId":13874099,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedGroupKFold, GroupKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.base import clone\n\nfrom tqdm import tqdm\nimport os\nimport random\nimport ipywidgets as widgets\nimport warnings\nimport json\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.597321Z","iopub.execute_input":"2025-11-16T18:51:56.598019Z","iopub.status.idle":"2025-11-16T18:51:56.602485Z","shell.execute_reply.started":"2025-11-16T18:51:56.597992Z","shell.execute_reply":"2025-11-16T18:51:56.601625Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class CFG:\n    train_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\"\n    test_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n    sample_submission_path = \"/kaggle/input/MABe-mouse-behavior-detection/sample_submission.csv\"\n    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n    output_path = \"/kaggle/working/\"\n    \n    mode = \"validate\"\n    # mode = \"submit\"\n    \n    n_splits = 3\n    cv = StratifiedGroupKFold(n_splits)\n    seed = 44\n    \n    activate_log = True  # for mlflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.603564Z","iopub.execute_input":"2025-11-16T18:51:56.603805Z","iopub.status.idle":"2025-11-16T18:51:56.621465Z","shell.execute_reply.started":"2025-11-16T18:51:56.603784Z","shell.execute_reply":"2025-11-16T18:51:56.620714Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# SEED \nos.environ[\"PYTHONHASHSEED\"] = str(CFG.seed) \nrnd = np.random.RandomState(CFG.seed)\nrandom.seed(CFG.seed)\nnp.random.seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.622659Z","iopub.execute_input":"2025-11-16T18:51:56.622941Z","iopub.status.idle":"2025-11-16T18:51:56.639159Z","shell.execute_reply.started":"2025-11-16T18:51:56.622924Z","shell.execute_reply":"2025-11-16T18:51:56.638585Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"train_df = pd.read_csv(CFG.train_path)\ntest_df = pd.read_csv(CFG.test_path)\nss_df = pd.read_csv(CFG.sample_submission_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.639761Z","iopub.execute_input":"2025-11-16T18:51:56.639941Z","iopub.status.idle":"2025-11-16T18:51:56.718851Z","shell.execute_reply.started":"2025-11-16T18:51:56.639925Z","shell.execute_reply":"2025-11-16T18:51:56.718236Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# Test data","metadata":{}},{"cell_type":"code","source":"# Meta data\ntest_df.iloc[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.720189Z","iopub.execute_input":"2025-11-16T18:51:56.720388Z","iopub.status.idle":"2025-11-16T18:51:56.726166Z","shell.execute_reply.started":"2025-11-16T18:51:56.720372Z","shell.execute_reply":"2025-11-16T18:51:56.725469Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"lab_id                                                   AdaptableSnail\nvideo_id                                                      438887472\nmouse1_strain                                                CD-1 (ICR)\nmouse1_color                                                      white\nmouse1_sex                                                         male\nmouse1_id                                                          13.0\nmouse1_age                                                   8-12 weeks\nmouse1_condition                                        wireless device\nmouse2_strain                                                CD-1 (ICR)\nmouse2_color                                                      white\nmouse2_sex                                                         male\nmouse2_id                                                          27.0\nmouse2_age                                                   8-12 weeks\nmouse2_condition                                        wireless device\nmouse3_strain                                                CD-1 (ICR)\nmouse3_color                                                      white\nmouse3_sex                                                         male\nmouse3_id                                                          41.0\nmouse3_age                                                   8-12 weeks\nmouse3_condition                                        wireless device\nmouse4_strain                                                CD-1 (ICR)\nmouse4_color                                                      white\nmouse4_sex                                                         male\nmouse4_id                                                          53.0\nmouse4_age                                                   8-12 weeks\nmouse4_condition                                        wireless device\nframes_per_second                                                  30.0\nvideo_duration_sec                                                614.7\npix_per_cm_approx                                                  16.0\nvideo_width_pix                                                    1214\nvideo_height_pix                                                   1090\narena_width_cm                                                     60.0\narena_height_cm                                                    60.0\narena_shape                                                      square\narena_type                                                     familiar\nbody_parts_tracked    [\"body_center\", \"ear_left\", \"ear_right\", \"head...\nbehaviors_labeled     [\"mouse1,mouse2,approach\", \"mouse1,mouse2,atta...\ntracking_method                                              DeepLabCut\nName: 0, dtype: object"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# Tracking data\ntest_tracking_df = pd.read_parquet(\"/kaggle/input/MABe-mouse-behavior-detection/test_tracking/AdaptableSnail/438887472.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.726969Z","iopub.execute_input":"2025-11-16T18:51:56.727177Z","iopub.status.idle":"2025-11-16T18:51:56.794187Z","shell.execute_reply.started":"2025-11-16T18:51:56.727162Z","shell.execute_reply":"2025-11-16T18:51:56.793528Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Submission \nss_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.795050Z","iopub.execute_input":"2025-11-16T18:51:56.795332Z","iopub.status.idle":"2025-11-16T18:51:56.803328Z","shell.execute_reply.started":"2025-11-16T18:51:56.795308Z","shell.execute_reply":"2025-11-16T18:51:56.802793Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"   row_id   video_id agent_id target_id action  start_frame  stop_frame\n0       0  438887472   mouse1    mouse2  sniff            0           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>video_id</th>\n      <th>agent_id</th>\n      <th>target_id</th>\n      <th>action</th>\n      <th>start_frame</th>\n      <th>stop_frame</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>438887472</td>\n      <td>mouse1</td>\n      <td>mouse2</td>\n      <td>sniff</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"code","source":"\"\"\"\nRemove MABe labs (missing)\nCount mice in per frame\n\"\"\"\nprint(f\"Original: {train_df.shape}\")\n\n# Remove 'MABe22_keypoints', 'MABe22_movies' from train data\nremove_list = [\"MABe22_keypoints\", \"MABe22_movies\"]\ntrain_without_mabe_df = train_df[~train_df[\"lab_id\"].isin(remove_list)].copy()\n\n# count mice in one frame (4 is MAX)\ntrain_without_mabe_df['n_mice'] = 4 - train_without_mabe_df[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].isna().sum(axis=1).copy()\n\nprint(f\"After removing MABe labs: {train_without_mabe_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.804114Z","iopub.execute_input":"2025-11-16T18:51:56.804369Z","iopub.status.idle":"2025-11-16T18:51:56.822324Z","shell.execute_reply.started":"2025-11-16T18:51:56.804352Z","shell.execute_reply":"2025-11-16T18:51:56.821561Z"}},"outputs":[{"name":"stdout","text":"Original: (8789, 38)\nAfter removing MABe labs: (863, 39)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## Integrate with Tracking data","metadata":{}},{"cell_type":"markdown","source":"1. Parquet の tracking データを読み込む（座標情報）\n2. bodypart（体の部位）を整理して不要部分を削除\n3. (mouse_id, bodypart) × frame の特徴行列に変換（ピボット）\n4. cm にスケール変換\n5. 行動アノテーション（JSON or parquet）を読み込む\n6. 単独行動（single mouse）/ ペア行動（pair of mice）の 特徴行列 + メタ + ラベル を 1サンプルずつ yield する","metadata":{}},{"cell_type":"code","source":"drop_body_parts =  [\n    'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n    'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n    'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n]\n\n# Only these body parts are used\n# [\n#  \"body_center\",\n#  \"ear_left\",\n#  \"ear_right\",\n#  \"lateral_left\",\n#  \"lateral_right\",\n#  \"neck\",\n#  \"nose\",\n#  \"tail_base\",\n#  \"tail_tip\"\n# ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.823139Z","iopub.execute_input":"2025-11-16T18:51:56.823385Z","iopub.status.idle":"2025-11-16T18:51:56.834601Z","shell.execute_reply.started":"2025-11-16T18:51:56.823363Z","shell.execute_reply":"2025-11-16T18:51:56.833953Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def generate_mouse_data(df, traintest, traintest_directory=None, generate_single=True, generate_pair=True):\n    \n    if traintest_directory is None:\n        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n        \n    for idx, row in df.iterrows():\n        lab_id = row.lab_id\n        if lab_id.startswith('MABe22') or type(row.behaviors_labeled) != str: \n            continue\n        \n        video_id = row.video_id\n        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n        vid = pd.read_parquet(path)\n        if len(np.unique(vid.bodypart)) > 5:\n            vid = vid[~vid[\"bodypart\"].isin(drop_body_parts)]\n        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n\n        del vid\n        gc.collect()\n        \n        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n        # frame | mouse1_head_x | mouse1_head_y | mouse1_nose_x | ... | mouse2_head_x | ...\n    \n        pvid /= row.pix_per_cm_approx  # pixcel to cm\n        # print(\"---------- pvid ----------\")\n        # print(pvid.columns)\n        # print()\n        vid_behaviors = json.loads(row.behaviors_labeled)\n        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n        vid_behaviors = [b.split(',') for b in vid_behaviors]\n        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n        # 'mouse1,self,walk', -> agent, target, action\n        # print(\"---------- vid_behaviors ----------\")\n        # print(vid_behaviors.columns)\n        # print()\n              \n        if traintest == 'train':\n            try:\n                annot = pd.read_parquet(path.replace('train_tracking', 'train_annotation'))\n            except FileNotFoundError:\n                continue\n\n        if generate_single:\n            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n            for mouse_id_str in np.unique(vid_behaviors_subset.agent):\n                try:\n                    mouse_id = int(mouse_id_str[-1])\n                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n                    single_mouse = pvid.loc[:, mouse_id]\n                    assert len(single_mouse) == len(pvid)\n                    single_mouse_meta = pd.DataFrame({\n                        'video_id': video_id,\n                        'agent_id': mouse_id_str,\n                        'target_id': 'self',\n                        'video_frame': single_mouse.index\n                    })\n                    if traintest == 'train':\n                        single_mouse_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=single_mouse.index)\n                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n                        for i in range(len(annot_subset)):\n                            annot_row = annot_subset.iloc[i]\n                            single_mouse_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n                        yield 'single', single_mouse, single_mouse_meta, single_mouse_label\n                    else:\n                        yield 'single', single_mouse, single_mouse_meta, vid_agent_actions\n                except KeyError:\n                    pass\n\n        if generate_pair:\n            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n            if len(vid_behaviors_subset) > 0:\n                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values('mouse_id')), 2): # int8\n                    agent_str = f\"mouse{agent}\"\n                    target_str = f\"mouse{target}\"\n                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n                    assert len(mouse_pair) == len(pvid)\n                    mouse_pair_meta = pd.DataFrame({\n                        'video_id': video_id,\n                        'agent_id': agent_str,\n                        'target_id': target_str,\n                        'video_frame': mouse_pair.index\n                    })\n                    if traintest == 'train':\n                        mouse_pair_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=mouse_pair.index)\n                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n                        for i in range(len(annot_subset)):\n                            annot_row = annot_subset.iloc[i]\n                            mouse_pair_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n                        yield 'pair', mouse_pair, mouse_pair_meta, mouse_pair_label\n                    else:\n                        yield 'pair', mouse_pair, mouse_pair_meta, vid_agent_actions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:44:16.290771Z","iopub.execute_input":"2025-11-16T19:44:16.291343Z","iopub.status.idle":"2025-11-16T19:44:16.304387Z","shell.execute_reply.started":"2025-11-16T19:44:16.291318Z","shell.execute_reply":"2025-11-16T19:44:16.303632Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"sample_df = train_without_mabe_df.iloc[[0]]\n\nsample_df\ngen = generate_mouse_data(sample_df, 'train')\n\n# Get the 1st item that generate_mouse_data() yields\nswitch, data, meta, label = next(gen)  \n\nprint(\"switch:\", switch)\nprint(\"data:\", data.head())\nprint(\"meta:\", meta.head())\nprint(\"label:\", label.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:56.853596Z","iopub.execute_input":"2025-11-16T18:51:56.853827Z","iopub.status.idle":"2025-11-16T18:51:57.830555Z","shell.execute_reply.started":"2025-11-16T18:51:56.853810Z","shell.execute_reply":"2025-11-16T18:51:57.829783Z"}},"outputs":[{"name":"stdout","text":"switch: single\ndata: bodypart    body_center            ear_left      ear_right             \\\n                      x          y        x   y          x          y   \nvideo_frame                                                             \n0             72.596497  32.694561      NaN NaN  71.644066  36.726250   \n1             72.667686  33.659126      NaN NaN  71.248749  37.480373   \n2             72.168686  34.761688      NaN NaN  71.167435  38.346626   \n3             72.593246  35.736687      NaN NaN  71.186996  39.507500   \n4             72.657684  36.620377      NaN NaN  71.246559  40.469189   \n\nbodypart    lateral_left            lateral_right            neck      \\\n                       x          y             x          y    x   y   \nvideo_frame                                                             \n0              73.903687  31.237312     70.724190  32.908939  NaN NaN   \n1              74.042686  31.809875     70.599503  33.544876  NaN NaN   \n2              73.838440  33.945126     70.520500  34.537750  NaN NaN   \n3              74.001434  34.808125     70.596001  35.068436  NaN NaN   \n4              74.220879  35.630249     70.793999  36.199501  NaN NaN   \n\nbodypart          nose             tail_base              tail_tip             \n                     x          y          x          y          x          y  \nvideo_frame                                                                    \n0                  NaN        NaN  71.379372  29.920500  68.158623  22.381750  \n1            73.620560  39.788311  71.322441  30.390312  67.826691  22.471937  \n2            73.302254  40.788937  71.819565  31.452999  68.704376  23.098812  \n3            73.448753  42.119499  72.244003  32.593498  69.157562  23.928562  \n4            73.640060  43.319561  72.186501  33.464249        NaN        NaN  \nmeta:    video_id agent_id target_id  video_frame\n0  44566106   mouse1      self            0\n1  44566106   mouse1      self            1\n2  44566106   mouse1      self            2\n3  44566106   mouse1      self            3\n4  44566106   mouse1      self            4\nlabel:              rear\nvideo_frame      \n0             0.0\n1             0.0\n2             0.0\n3             0.0\n4             0.0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"body_parts_tracked_list = list(np.unique(train_without_mabe_df.body_parts_tracked))\nbody_parts_tracked_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:57.837515Z","iopub.execute_input":"2025-11-16T18:51:57.837700Z","iopub.status.idle":"2025-11-16T18:51:57.852576Z","shell.execute_reply.started":"2025-11-16T18:51:57.837685Z","shell.execute_reply":"2025-11-16T18:51:57.851976Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"['[\"body_center\", \"ear_left\", \"ear_right\", \"headpiece_bottombackleft\", \"headpiece_bottombackright\", \"headpiece_bottomfrontleft\", \"headpiece_bottomfrontright\", \"headpiece_topbackleft\", \"headpiece_topbackright\", \"headpiece_topfrontleft\", \"headpiece_topfrontright\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_midpoint\", \"tail_tip\"]',\n '[\"body_center\", \"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"spine_1\", \"spine_2\", \"tail_base\", \"tail_middle_1\", \"tail_middle_2\", \"tail_tip\"]',\n '[\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_midpoint\", \"tail_tip\"]',\n '[\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\", \"tail_tip\"]',\n '[\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\"]',\n '[\"body_center\", \"ear_left\", \"ear_right\", \"nose\", \"tail_base\"]',\n '[\"ear_left\", \"ear_right\", \"head\", \"tail_base\"]',\n '[\"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"neck\", \"nose\", \"tail_base\"]',\n '[\"ear_left\", \"ear_right\", \"nose\", \"tail_base\", \"tail_tip\"]']"},"metadata":{}}],"execution_count":47},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"# None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:57.853253Z","iopub.execute_input":"2025-11-16T18:51:57.853485Z","iopub.status.idle":"2025-11-16T18:51:57.866255Z","shell.execute_reply.started":"2025-11-16T18:51:57.853462Z","shell.execute_reply":"2025-11-16T18:51:57.865675Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport xgboost as xgb\nimport itertools\n\n\n#  threshold optimization (binary F1)\ndef optimize_threshold(pred, true):\n    best_f1 = 0\n    best_th = 0.5\n    for th in np.linspace(0.05, 0.95, 19):\n        f1 = f1_score(true, (pred >= th).astype(int), zero_division=0)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_th = th\n    return best_th\n\n\n#  Cross validation\ndef cross_validate_xgb(X, y_df, meta_df, tracking_key):\n\n    meta_df = meta_df.set_index(\"video_frame\")\n    X.index = meta_df.index\n    y_df.index = meta_df.index\n\n    oof = pd.DataFrame(index=meta_df.index)\n    thresholds = {}\n    f1_results = []\n\n    for action in y_df.columns:\n\n        valid_mask = ~y_df[action].isna()\n        y = y_df[action][valid_mask].astype(float).values\n        X_action = X.loc[valid_mask]\n        groups = meta_df.video_id[valid_mask]\n\n        if y.sum() == 0:\n            print(f\"Action {action}: all negatives -> skipped\")\n            oof[action] = np.zeros(len(meta_df))\n            thresholds[action] = 1.0\n            continue\n\n        gkf = GroupKFold(CFG.n_splits)\n        oof_pred = np.zeros(len(y))\n\n        for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_action, y, groups)):\n\n            train_data = xgb.DMatrix(X_action.iloc[tr_idx], label=y[tr_idx])\n            valid_data = xgb.DMatrix(X_action.iloc[va_idx], label=y[va_idx])\n\n            model = xgb.train(\n                params=dict(\n                    objective='binary:logistic',\n                    eval_metric='logloss',\n                    max_depth=6,\n                    eta=0.05,\n                    subsample=0.8,\n                    colsample_bytree=0.8,\n                ),\n                dtrain=train_data,\n                num_boost_round=500,\n                early_stopping_rounds=50,\n                evals=[(valid_data, 'valid')],\n                verbose_eval=False\n            )\n\n            oof_pred[va_idx] = model.predict(valid_data)\n\n            model.save_model(\n                f\"xgb_model_{tracking_key}_{action}_fold{fold}.json\"\n            )\n\n        best_th = optimize_threshold(oof_pred, y)\n        thresholds[action] = best_th\n\n        f1 = f1_score(y, (oof_pred >= best_th).astype(int), zero_division=0)\n        f1_results.append((action, f1))\n\n        full_pred = np.zeros(len(meta_df))\n        full_pred[valid_mask] = oof_pred\n        oof[action] = full_pred\n        \n        print(f\"Action {action}: F1={f1:.4f}, threshold={best_th:.2f}\")\n\n    return oof, f1_results, thresholds\n\n\n#  Prediction -> convert to submission\ndef predict_multiclass(pred, meta_df, thresholds):\n\n    ama = pred.values.argmax(axis=1)\n    max_proba = pred.values.max(axis=1)\n\n    th_array = np.array([thresholds.get(action, 0.5) for action in pred.columns])\n    pass_mask = max_proba >= th_array[ama]\n\n    ama = np.where(pass_mask, ama, -1)\n    ama = pd.Series(ama, index=meta_df.index)\n\n    changes = ama != ama.shift(1)\n    ama_changes = ama[changes]\n    meta_changes = meta_df[changes]  \n\n    mask = ama_changes >= 0\n    if len(mask) > 0:\n        mask.iloc[-1] = False\n\n    submission = pd.DataFrame({\n        \"video_id\": meta_changes.video_id[mask].values,\n        \"agent_id\": meta_changes.agent_id[mask].values,\n        \"target_id\": meta_changes.target_id[mask].values,\n        \"action\": pred.columns[ama_changes[mask].values],\n        \"start_frame\": ama_changes.index[mask],\n        \"stop_frame\": ama_changes.index[1:][mask[:-1]],\n    })\n\n    return submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:51:57.867045Z","iopub.execute_input":"2025-11-16T18:51:57.867321Z","iopub.status.idle":"2025-11-16T18:51:57.884081Z","shell.execute_reply.started":"2025-11-16T18:51:57.867306Z","shell.execute_reply":"2025-11-16T18:51:57.883465Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def normalize_tracking_str(raw_str):\n    parts = json.loads(raw_str)\n    parts = [p for p in parts if p not in drop_body_parts]  # drop unwanted\n    return json.dumps(sorted(parts))\n\ntrain_without_mabe_df[\"body_parts_tracked\"] = (\n    train_without_mabe_df[\"body_parts_tracked\"].apply(normalize_tracking_str)\n)\n\ntest_df[\"body_parts_tracked\"] = (\n    test_df[\"body_parts_tracked\"].apply(normalize_tracking_str)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:50:55.279490Z","iopub.execute_input":"2025-11-16T19:50:55.279811Z","iopub.status.idle":"2025-11-16T19:50:55.290783Z","shell.execute_reply.started":"2025-11-16T19:50:55.279790Z","shell.execute_reply":"2025-11-16T19:50:55.290235Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"\"\"\"\nTrain\n\"\"\"\nall_submissions = []\nall_f1 = []\nall_thresholds = {}\n\nbody_parts_tracked_list = sorted(train_without_mabe_df.body_parts_tracked.unique())\n\n# Different videos have different sets of tracked body parts.\n# Therefore, need to train separate models for each tracking set.\nfor section in range(1, len(body_parts_tracked_list)):\n    \n    raw_str = body_parts_tracked_list[section]\n    body_parts = json.loads(raw_str)\n    body_parts_sorted_str = json.dumps(sorted(body_parts))  # to unify the order with test data\n    \n    print(\"=\"*30)\n    print(f\"{section}/{len(body_parts_tracked_list)-1} Processing videos with: {body_parts_sorted_str}\\n\")\n\n    train_subset = train_without_mabe_df[\n        train_without_mabe_df.body_parts_tracked == raw_str\n    ]\n\n    single_tracking_data = []\n    single_label = []\n    single_meta = []\n\n    pair_tracking_data = []\n    pair_label = []\n    pair_meta = []\n\n    # Pre-processing for train data\n    for switch, tracking_data, meta, label in generate_mouse_data(train_subset, \"train\"):\n        tracking_data = tracking_data.astype(np.float32)\n\n        if switch == \"single\":\n            single_tracking_data.append(tracking_data)\n            single_label.append(label)\n            single_meta.append(meta)\n        else:\n            pair_tracking_data.append(tracking_data)\n            pair_label.append(label)\n            pair_meta.append(meta)\n\n        del tracking_data, meta, label\n    gc.collect()\n\n    # =========================\n    # Single training\n    # =========================\n    if len(single_tracking_data) > 0:\n        X = pd.concat(single_tracking_data, ignore_index=True)\n        y_df = pd.concat(single_label, ignore_index=True)\n        meta_df = pd.concat(single_meta, ignore_index=True)\n\n        if \"video_frame\" not in meta_df.columns:\n            print(\"[ERROR] video_frame column missing\")\n            print(meta_df.head())\n            continue\n\n        oof, f1_result, th = cross_validate_xgb(\n            X, y_df, meta_df, tracking_key=body_parts_sorted_str\n        )\n        all_f1.extend(f1_result)\n        all_thresholds[(body_parts_sorted_str, \"single\")] = th\n        sub = predict_multiclass(oof, meta_df.set_index(\"video_frame\"), th)\n        all_submissions.append(sub)\n\n    # =========================\n    # Pair training (optional)\n    # =========================\n    if len(pair_tracking_data) > 0:\n        X = pd.concat(pair_tracking_data, ignore_index=True)\n        y_df = pd.concat(pair_label, ignore_index=True)\n        meta_df = pd.concat(pair_meta, ignore_index=True)\n\n        if \"video_frame\" not in meta_df.columns:\n            print(\"[ERROR] video_frame column missing\")\n            print(meta_df.head())\n            continue\n\n        oof, f1_result, th = cross_validate_xgb(\n            X, y_df, meta_df, tracking_key=body_parts_sorted_str\n        )\n        all_f1.extend(f1_result)\n        all_thresholds[(body_parts_sorted_str, \"pair\")] = th\n        sub = predict_multiclass(oof, meta_df.set_index(\"video_frame\"), th)\n        all_submissions.append(sub)\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:51:00.765824Z","iopub.execute_input":"2025-11-16T19:51:00.766370Z","iopub.status.idle":"2025-11-16T20:08:41.256723Z","shell.execute_reply.started":"2025-11-16T19:51:00.766347Z","shell.execute_reply":"2025-11-16T20:08:41.255969Z"}},"outputs":[{"name":"stdout","text":"==============================\n1/7 Processing videos with: [\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_tip\"]\n\nAction rear: F1=0.3027, threshold=0.15\n\n==============================\n2/7 Processing videos with: [\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\", \"tail_tip\"]\n\n\n==============================\n3/7 Processing videos with: [\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"nose\", \"tail_base\"]\n\n\n==============================\n4/7 Processing videos with: [\"body_center\", \"ear_left\", \"ear_right\", \"nose\", \"tail_base\"]\n\nAction biteobject: F1=0.0012, threshold=0.10\nAction climb: F1=0.4412, threshold=0.15\nAction dig: F1=0.3749, threshold=0.20\nAction exploreobject: F1=0.1434, threshold=0.05\nAction rear: F1=0.1763, threshold=0.10\nAction selfgroom: F1=0.2214, threshold=0.10\n\n==============================\n5/7 Processing videos with: [\"ear_left\", \"ear_right\", \"head\", \"tail_base\"]\n\nAction rear: F1=0.6347, threshold=0.30\nAction rest: F1=0.4068, threshold=0.10\nAction selfgroom: F1=0.1228, threshold=0.05\nAction climb: F1=0.4708, threshold=0.15\nAction dig: F1=0.2408, threshold=0.15\nAction run: F1=0.0048, threshold=0.05\n\n==============================\n6/7 Processing videos with: [\"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"neck\", \"nose\", \"tail_base\"]\n\nAction rear: F1=0.1495, threshold=0.15\nAction selfgroom: F1=0.1626, threshold=0.10\nAction genitalgroom: F1=0.3278, threshold=0.05\nAction dig: F1=0.0951, threshold=0.10\n\n==============================\n7/7 Processing videos with: [\"ear_left\", \"ear_right\", \"nose\", \"tail_base\", \"tail_tip\"]\n\nAction freeze: F1=0.2391, threshold=0.05\nAction rear: F1=0.4040, threshold=0.20\n\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"# Inference & Submission","metadata":{}},{"cell_type":"code","source":"def submit_XGB(test_df, all_thresholds, n_folds=CFG.n_splits):\n\n    submission_list = []\n\n    body_parts_tracked_list = sorted(test_df.body_parts_tracked.unique())\n    \n    print(body_parts_tracked_list)\n    \n    for section, body_parts_tracked_str in enumerate(body_parts_tracked_list):\n\n        print(f\"[Test] Processing tracking set {section}: {body_parts_tracked_str}\")\n\n        test_subset = test_df[test_df.body_parts_tracked == body_parts_tracked_str]\n\n        generator = generate_mouse_data(\n            test_subset,\n            'test',\n            generate_single=True,\n            generate_pair=True\n        )\n            \n        for switch, data_te, meta_te, actions_te in generator:\n\n            video_id = meta_te.video_id.iloc[0]\n            print(f\"video_id={video_id}, switch={switch}\")\n\n            # ---- sorted key ----\n            body_parts = json.loads(body_parts_tracked_str)\n            body_parts_sorted_str = json.dumps(sorted(body_parts))\n            \n            # ---- sorted keyで検索 ----\n            th = all_thresholds.get((body_parts_sorted_str, switch), None)\n            print(th)\n            if th is None:\n                print(\"No thresholds, skipping...\")\n                continue\n\n            X_te = data_te.astype(np.float32)\n            dtest = xgb.DMatrix(X_te)\n\n            pred = pd.DataFrame(index=meta_te.video_frame)\n\n            for action in actions_te:\n\n                fold_preds = []\n                for fold in range(n_folds):\n\n                    model_path = f\"xgb_model_{body_parts_sorted_str}_{action}_fold{fold}.json\"\n                    \n                    if not os.path.exists(model_path):\n                        continue\n\n                    model = xgb.Booster()\n                    model.load_model(model_path)\n\n                    fold_pred = model.predict(dtest)\n                    fold_preds.append(fold_pred)\n\n                if len(fold_preds) == 0:\n                    print(\"Model is not found\")\n                    continue\n\n                avg_pred = np.mean(fold_preds, axis=0)\n                pred[action] = avg_pred\n\n            if pred.shape[1] == 0:\n                continue\n\n            submission_part = predict_multiclass(pred, meta_te.set_index(\"video_frame\"), th)\n            submission_list.append(submission_part)\n\n    if len(submission_list) == 0:\n        print(\"No submission data produced!\")\n        return None\n\n    submission = pd.concat(submission_list, ignore_index=True)\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"submission.csv saved!\")\n\n    return submission\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T20:08:41.264187Z","iopub.execute_input":"2025-11-16T20:08:41.264435Z","iopub.status.idle":"2025-11-16T20:08:41.281571Z","shell.execute_reply.started":"2025-11-16T20:08:41.264414Z","shell.execute_reply":"2025-11-16T20:08:41.280803Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"submit_XGB(test_df, all_thresholds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T20:09:31.961770Z","iopub.execute_input":"2025-11-16T20:09:31.962356Z","iopub.status.idle":"2025-11-16T20:09:32.964194Z","shell.execute_reply.started":"2025-11-16T20:09:31.962331Z","shell.execute_reply":"2025-11-16T20:09:32.963408Z"}},"outputs":[{"name":"stdout","text":"['[\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_tip\"]']\n[Test] Processing tracking set 0: [\"body_center\", \"ear_left\", \"ear_right\", \"lateral_left\", \"lateral_right\", \"neck\", \"nose\", \"tail_base\", \"tail_tip\"]\nvideo_id=438887472, switch=single\n{'rear': 0.15}\nModel is not found\nvideo_id=438887472, switch=single\n{'rear': 0.15}\nModel is not found\nvideo_id=438887472, switch=single\n{'rear': 0.15}\nModel is not found\nvideo_id=438887472, switch=single\n{'rear': 0.15}\nModel is not found\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nvideo_id=438887472, switch=pair\nNone\nNo thresholds, skipping...\nNo submission data produced!\n","output_type":"stream"}],"execution_count":73}]}